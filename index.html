<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Open-Vocabulary 3D Semantic Segmentation with Text-to-Image Diffusion Models</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>



  
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Open-Vocabulary 3D Semantic Segmentation with Text-to-Image Diffusion Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://zgzxy001.github.io/" target="_blank">Xiaoyu Zhu</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://zhhoper.github.io/" target="_blank">Hao Zhou</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://zhhoper.github.io/" target="_blank">Hao Zhou</a><sup>2</sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://garyzhao.github.io/" target="_blank">Long Zhao</a><sup>2</sup>,</span>
                    <span class="author-block">
                  <span class="author-block">
                        <a href="https://diff2scene.github.io/" target="_blank">Hao Xu</a><sup>3</sup>,</span>
                  <span class="author-block">
                <span class="author-block">
                      <a href="https://junweiliang.me/" target="_blank">Junwei Liang</a><sup>4</sup>,</span>
                <span class="author-block">
                  <span class="author-block">
                    <a href="https://scholar.google.co.uk/citations?user=Py54GcEAAAAJ&hl=en" target="_blank">Alexander Hauptmann</a><sup>1</sup>,</span>
                <span class="author-block">
                  <span class="author-block">
                    <a href="http://www.tliu.org/" target="_blank">Ting Liu</a><sup>2</sup>,</span>
                <span class="author-block">
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=-RFj-kYAAAAJ&hl=en" target="_blank">Andrew Gallagher</a><sup>2</sup>,</span>
                <span class="author-block">
                  </div>


            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>CMU,</span>
              <span class="author-block"><sup>2</sup>Google DeepMind,</span>
              <span class="author-block"><sup>2</sup>Google,</span>
              <span class="author-block"><sup>2</sup>HKUST</span>
              <span class="eql-cntrb"><small></small></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">ECCV 2024</span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2407.13642.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>



            <!-- ArXiv abstract Link -->
            <span class="link-block">
              <a href="https://drive.google.com/file/d/1Dgm_OMXkjXVp1u_JAxOn0LJ9yQSTkych/view?usp=sharing" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>Poster</span>
              </a>
            </span>
            <span class="link-block">
              <a href="https://www.youtube.com/watch?v=8ZwrrLXH8No&t=8s" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>Youtube</span>
              </a>
            </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>
</section>



<!-- Teaser video-->

<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In this paper, we investigate the use of diffusion models which are pre-trained on large-scale image-caption pairs for open-vocabulary 3D semantic understanding. We propose a novel method, namely Diff2Scene, which leverages frozen representations from text-image generative models, along with salient-aware and geometric-aware masks, for open-vocabulary 3D semantic segmentation and visual grounding tasks. Diff2Scene gets rid of any labeled 3D data and effectively identifies objects, appearances, materials, locations and their compositions in 3D scenes. We show that it outperforms competitive baselines and achieves significant improvements over state-of-the-art methods. In particular, Diff2Scene improves the state-of-the-art method on ScanNet200 by 12%.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Open-Vocabulary 3D Perception-Problem Definition</h2>
          <center>
          <img src="static/images/fig1.png" alt="Problem definition" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              Illustration of open-vocabulary 3D semantic scene understanding. We propose Diff2Scene, a 3D model that performs open-vocabulary semantic segmentation and visual grounding tasks given novel text prompts, without relying on any annotated 3D data. By leveraging discriminative-based and generative-based 2D foundation models, Diff2Scene can handle a wide variety of novel text queries for both common and rare classes, like “desk” and “soap dispenser”. It can also handle compositional queries, such as “find the white sneakers that are closer to the desk chair.”            
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Open-Vocabulary 3D Perception-Method Overview</h2>
          <center>
          <img src="static/images/method_overview_v11-1.png" alt="Problem definition" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              Illustration of open-vocabulary 3D perception methods. (a) Directly minimizing the per-point feature distance between the CLIP-based model and the tuned 3D model. (b) Directly using a 3D mask proposal network trained on labeled 3D data to produce class-agnostic masks, and then pool corresponding representations from the CLIP feature map. (c) The proposed mask distillation approach, namely Diff2Scene, that uses Stable Diffusion and performs mask-based distillation. Diff2Scene leverages the semantically-rich mask embeddings from 2D foundation models and geometrically accurate masks from the tuned 3D model, and thus achieves superior performance compared to previous methods.            
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Method</h2>
          <center>
          <img src="static/images/open3d_method_v14.png" alt="Problem definition" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              Overview of our method. We propose Diff2Scene, an open-vocabulary 3D semantic understanding model. Diff2Scene contains two branches. The 2D branch is designed to be a diffusion-based 2D semantic segmentation model. It accepts a 2D image as input and predicts a set of 2D probabilistic masks with corresponding semantically-rich mask embeddings. The 3D branch utilizes the point cloud and 2D mask embeddings as input. The 2D mask embeddings are used as “semantic queries” to generate corresponding 3D probabilistic masks. The model learns salient patterns from the RGB images and geometric information from the point clouds.            
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Qualitative Results on Zero-Shot Semantic Segmentation</h2>
          <center>
          <img src="static/images/vis_seg.png" alt="vis seg" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              Qualitative results from our model and OpenScene on zero-shot semantic segmentation. We observe that our model can predict coherent masks with accurate semantic labels compared to OpenScene for both head and tail categories.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Qualitative Results on Zero-Shot Visual Grounding</h2>
          <center>
          <img src="static/images/vis_ground.png" alt="vis grounding" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              Qualitative results from our model and OpenScene on zero-shot visual grounding. Our open-vocabulary semantic understanding model is capable of handling different types of novel and compositional queries. Novel object classes as well as objects described by colors, shapes, appearances, locations, and usages are successfully retrieved by our method. Note that the located points are colored in yellow.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>








<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{zhu2024open,
        author    = {Zhu, Xiaoyu and Zhou, Hao and Xing, Pengfei and Zhao, Long and Xu, Hao and Liang, Junwei and Hauptmann, Alexander and Liu, Ting and Gallagher, Andrew},
        title     = {Open-Vocabulary 3D Semantic Segmentation with Text-to-Image Diffusion Models},
        booktitle = {European Conference on Computer Vision (ECCV)},
        year      = {2024}
    }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            If you have any questions or would like to discuss potential future work, please feel free to send an email to xiaoyuz3@cs.cmu.edu.
          </p>
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page. This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
